# Transformer_from_Scratch
An 'under-the-hood' implementation of the Transformer model (Vaswani et al., 2017) from scratch to understand its core mathematical foundations.
# Transformer from Scratch (Work in Progress)
THIS PROJECT IS NOT WRITTEN/GENERATED BY AI.

**Project Goal:** To build a vanilla Transformer (from the paper "Attention Is All You Need") from scratch, using only PyTorch and NumPy.

**Motivation:**
My primary goal for this project is not just to build a model, but to deeply understand the underlying mathematics and linear algebra that make it work. I am particularly interested in:
* The matrix operations behind Scaled Dot-Product Attention.
* The mathematical understanding and intuition for Positional Encodings.
* The role of residual connections and layer normalization in gradient flow.

**My Plan / Next Steps:**
1.  **[Completed]** Implement the `PositionalEncoding` block.
2.  **[In Progress]** Build the `ScaledDotProductAttention` mechanism.
3.  **[To-Do]** Combine this into the `MultiHeadAttention` block.
4.  **[To-Do]** Build the full Encoder block.
5.  **[To-Do]** Build the full Decoder block.
6.  **[To-Do]** Combine into the final Encoder-Decoder model.

This is a self-study project I am actively working on.
